{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pii_detection.probe_labels_utils import read_tokens_df\n",
    "\n",
    "df_train = read_tokens_df(\"../data/train_shard.json\")\n",
    "df_val = read_tokens_df(\"../data/val_shard.json\")\n",
    "\n",
    "\n",
    "\n",
    "def containes_dot(word: str):\n",
    "    if \".\" in word and not word.startswith(\".\") and not word.endswith(\".\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def digit_streak_in_str(word:str):\n",
    "    max_streak = 0\n",
    "    streak = 0\n",
    "    for c in word:\n",
    "        if c.isdigit():\n",
    "            streak += 1\n",
    "        else:\n",
    "            max_streak = max(max_streak, streak)\n",
    "            streak = 0\n",
    "    return max_streak\n",
    "\n",
    "for df in [df_train, df_val]:\n",
    "    df[\"contains_dot\"] = df[\"tokens\"].apply(containes_dot)\n",
    "    df[\"contains_(\"] = df[\"tokens\"].str.contains(\"(\", regex=False)\n",
    "    df[\"contains_-\"] = df[\"tokens\"].str.contains(\"-\", regex=False)\n",
    "    df[\"contains_digit\"] = df[\"tokens\"].apply(lambda x: any([c.isdigit() for c in x]))\n",
    "    df[\"contains_@\"] = df[\"tokens\"].str.contains(\"@\", regex=False)\n",
    "    df[\"contains_http\"] = df[\"tokens\"].str.contains(\"http\", regex=False)\n",
    "    df[\"contains_www\"] = df[\"tokens\"].str.contains(\"www\", regex=False)\n",
    "    df[\"contains_.com\"] = df[\"tokens\"].str.contains(\".com\", regex=False)\n",
    "    df[\"is_upper\"] = df[\"tokens\"].str.isupper()\n",
    "    df[\"is_title\"] = df[\"tokens\"].str.istitle()\n",
    "    df[\"is_digit\"] = df[\"tokens\"].str.isdigit()\n",
    "    df[\"is_alpha\"] = df[\"tokens\"].str.isalpha()\n",
    "    df[\"is_space\"] = df[\"tokens\"].str.isspace()\n",
    "    df[\"is_lower\"] = df[\"tokens\"].str.islower()\n",
    "    df[\"is_numeric\"] = df[\"tokens\"].str.isnumeric()\n",
    "    df[\"is_alnum\"] = df[\"tokens\"].str.isalnum()\n",
    "    df[\"is_decimal\"] = df[\"tokens\"].str.isdecimal()\n",
    "    df[\"tokens_len\"] = df[\"tokens\"].str.len()\n",
    "    df[\"new_line\"] = df[\"tokens\"].str.contains(\"\\n\")\n",
    "    df[\"new_paragraph\"] = (df[\"tokens\"].str.count(\"\\n\") > 1)\n",
    "    df[\"line_num\"] = df.groupby(\"document\")[\"new_line\"].cumsum()\n",
    "    df[\"paragraph_num\"] = df.groupby(\"document\")[\"new_paragraph\"].cumsum()\n",
    "    df[\"n_digits\"] = df[\"tokens\"].apply(lambda x: sum([c.isdigit() for c in x]))\n",
    "    df[\"digit_streak\"] = df[\"tokens\"].apply(digit_streak_in_str)\n",
    "# df[~df[\"tokens\"].apply(f)][\"labels\"].value_counts()\n",
    "# df[\"tokens\"].apply(f).value_counts(normalize=True)\n",
    "\n",
    "df_train[\"labels\"] = df_train[\"labels\"].apply(lambda x: \"O\" if \"NAME\" in x else x)\n",
    "df_val[\"labels\"] = df_val[\"labels\"].apply(lambda x: \"O\" if \"NAME\" in x else x)\n",
    "df_train.shape[1] == df_val.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def mask(df: pd.DataFrame) -> pd.Series:\n",
    "    return (df[\"is_title\"] | df[\"contains_http\"] | df[\"contains_www\"] | df[\"contains_.com\"] | df[\"contains_@\"] | df[\"contains_(\"] | df[\"contains_-\"] | df[\"contains_digit\"] | df[\"contains_dot\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train random forest classifier:\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "train_mask = mask(df_train)\n",
    "val_mask = mask(df_val)\n",
    "\n",
    "X_train = df_train[train_mask].drop([\"labels\", \"tokens\", \"document\"], axis=1)\n",
    "y_train = df_train[train_mask][\"labels\"]\n",
    "X_val = df_val[val_mask].drop([\"labels\", \"tokens\", \"document\"], axis=1)\n",
    "y_val = df_val[val_mask][\"labels\"]\n",
    "\n",
    "clf = RandomForestClassifier(verbose=3, n_jobs=8)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(np.unique(y_pred))\n",
    "print(np.unique(y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_train)\n",
    "print(classification_report(y_train, y_pred))\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print(np.unique(y_pred))\n",
    "print(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/train_shard.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "train_data = sorted(train_data, key=lambda x: x[\"document\"])\n",
    "\n",
    "all_names = []\n",
    "for essay in train_data:\n",
    "    doc_id = essay[\"document\"]\n",
    "    names = []\n",
    "    \n",
    "    for token, label in zip(essay[\"tokens\"], essay[\"labels\"]):\n",
    "        if \"NAME_STUDENT\" in label:\n",
    "            if label.startswith(\"B\"):\n",
    "                names.append(token)\n",
    "            elif label.startswith(\"I\"):\n",
    "                names[-1] += f\" {token}\"\n",
    "    if names:\n",
    "        all_names.append(names)\n",
    "\n",
    "all_names\n",
    "\n",
    "\n",
    "with open(\"../data/train_shard_renamed.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "train_data = sorted(train_data, key=lambda x: x[\"document\"])\n",
    "\n",
    "all_names2 = []\n",
    "for essay in train_data:\n",
    "    doc_id = essay[\"document\"]\n",
    "    names = []\n",
    "    \n",
    "    for token, label in zip(essay[\"tokens\"], essay[\"labels\"]):\n",
    "        if \"NAME_STUDENT\" in label:\n",
    "            if label.startswith(\"B\"):\n",
    "                names.append(token)\n",
    "            elif label.startswith(\"I\"):\n",
    "                names[-1] += f\" {token}\"\n",
    "    if names:\n",
    "        all_names2.append(names)\n",
    "\n",
    "pd.DataFrame({\"names\": all_names, \"names2\": all_names2})."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/train_shard_renamed.json\", \"r\") as f:\n",
    "    train_data = sorted(json.load(f), key=lambda x: x[\"document\"])\n",
    "\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "# import bert-base-uncased\n",
    "\n",
    "def tokenize_text(text, max_length=512):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False, max_length=max_length)\n",
    "    return tokenizer.convert_ids_to_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "def chat_with_bert(context, question, model, tokenizer):\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(context, question, return_tensors=\"pt\")\n",
    "\n",
    "    # Get the model output\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # Get the answer span\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "\n",
    "    # Find the tokens with the highest start and end scores\n",
    "    start_index = torch.argmax(start_scores)\n",
    "    end_index = torch.argmax(end_scores)\n",
    "\n",
    "    # Get the answer span\n",
    "    answer_span = inputs[\"input_ids\"][0][start_index:end_index+1]\n",
    "    answer = tokenizer.decode(answer_span, skip_special_tokens=True)\n",
    "\n",
    "    return answer\n",
    "\n",
    "# Load DistilBERT model and tokenizer\n",
    "model_name = 'distilbert-base-cased-distilled-squad'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Example conversation\n",
    "context = \"BERT is a powerful pre-trained language model. It can be fine-tuned for various natural language processing tasks.\"\n",
    "\n",
    "# Chat with BERT\n",
    "while True:\n",
    "    question = input(\"You: \")\n",
    "    if question.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "        print(\"Chat ended.\")\n",
    "        break\n",
    "    \n",
    "    answer = chat_with_bert(context, question, model, tokenizer)\n",
    "    print(\"BERT:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
