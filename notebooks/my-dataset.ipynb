{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bb8ec81",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-04T15:47:21.380172Z",
     "iopub.status.busy": "2024-05-04T15:47:21.379437Z",
     "iopub.status.idle": "2024-05-04T15:47:29.846174Z",
     "shell.execute_reply": "2024-05-04T15:47:29.844884Z"
    },
    "papermill": {
     "duration": 8.474027,
     "end_time": "2024-05-04T15:47:29.848831",
     "exception": false,
     "start_time": "2024-05-04T15:47:21.374804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from itertools import chain\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "TRAINING_MODEL_PATH = \"microsoft/deberta-v3-base\"\n",
    "TRAINING_MAX_LENGTH = 1024\n",
    "STRIDE=384\n",
    "\n",
    "\n",
    "data = json.load(Path(\"/kaggle/input/pii-detection-removal-from-educational-data/train.json\").open(\"r\"))\n",
    "\n",
    "all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n",
    "label2id = {l: i for i,l in enumerate(all_labels)}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039fa4ab",
   "metadata": {
    "papermill": {
     "duration": 0.002272,
     "end_time": "2024-05-04T15:47:29.854256",
     "exception": false,
     "start_time": "2024-05-04T15:47:29.851984",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "266b9428",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:47:29.860881Z",
     "iopub.status.busy": "2024-05-04T15:47:29.860370Z",
     "iopub.status.idle": "2024-05-04T15:47:29.869572Z",
     "shell.execute_reply": "2024-05-04T15:47:29.868195Z"
    },
    "papermill": {
     "duration": 0.014953,
     "end_time": "2024-05-04T15:47:29.871654",
     "exception": false,
     "start_time": "2024-05-04T15:47:29.856701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def tokenize(example, tokenizer, label2id, max_length):\n",
    "\n",
    "    # rebuild text from tokens\n",
    "    text = []\n",
    "    labels = []\n",
    "\n",
    "    for t, l, ws in zip(\n",
    "        example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]\n",
    "    ):\n",
    "        text.append(t)\n",
    "        labels.extend([l] * len(t))\n",
    "\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            labels.append(\"O\")\n",
    "\n",
    "    # actual tokenization\n",
    "    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, max_length=max_length, stride=STRIDE, truncation=True)\n",
    "\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    text = \"\".join(text)\n",
    "    token_labels = []\n",
    "\n",
    "    for start_idx, end_idx in tokenized.offset_mapping:\n",
    "        # CLS token\n",
    "        if start_idx == 0 and end_idx == 0:\n",
    "            token_labels.append(label2id[\"O\"])\n",
    "            continue\n",
    "\n",
    "        # case when token starts with whitespace\n",
    "        if text[start_idx].isspace():\n",
    "            start_idx += 1\n",
    "\n",
    "        token_labels.append(label2id[labels[start_idx]])\n",
    "\n",
    "   \n",
    "    length = len(tokenized.input_ids)\n",
    "    return {**tokenized, \"labels\": token_labels, \"length\": length}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468873a3",
   "metadata": {
    "papermill": {
     "duration": 0.002269,
     "end_time": "2024-05-04T15:47:29.876517",
     "exception": false,
     "start_time": "2024-05-04T15:47:29.874248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Paragraph Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd2db83e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:47:29.883121Z",
     "iopub.status.busy": "2024-05-04T15:47:29.882416Z",
     "iopub.status.idle": "2024-05-04T15:47:29.894110Z",
     "shell.execute_reply": "2024-05-04T15:47:29.892784Z"
    },
    "papermill": {
     "duration": 0.017461,
     "end_time": "2024-05-04T15:47:29.896367",
     "exception": false,
     "start_time": "2024-05-04T15:47:29.878906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_tokens_to_text(tokens, whitespaces):\n",
    "    text = []\n",
    "    for token, whitespace in zip(tokens, whitespaces):\n",
    "        text.append(token)\n",
    "        if whitespace:\n",
    "            text.append(\" \")\n",
    "    return \"\".join(text)\n",
    "\n",
    "def partial_essay(essay):\n",
    "    if 'augmented' in essay:\n",
    "        return essay\n",
    "    paragraph_inices = [i+1 for i, pair in enumerate(list(zip(essay[\"tokens\"][:-1], essay[\"tokens\"][1:]))) if pair == (\".\", \"\\n\\n\")]\n",
    "    paragraphs = [{\"full_text\": convert_tokens_to_text(essay[\"tokens\"][i:j], essay[\"trailing_whitespace\"][i:j]),\n",
    "                   \"document\": essay[\"document\"],\n",
    "                   \"tokens\": essay[\"tokens\"][i:j],\n",
    "                   \"trailing_whitespace\": essay[\"trailing_whitespace\"][i:j],\n",
    "                   \"labels\": essay[\"labels\"][i:j]} for i, j in zip([0]+paragraph_inices, paragraph_inices+[len(essay[\"tokens\"])])]\n",
    "    \n",
    "    if len(paragraphs) < 3:\n",
    "        return essay\n",
    "    \n",
    "    middle_paragraphs = [par for par in paragraphs[1:-1] if np.random.random() < 0.5]\n",
    "    chosen_paragraphs = [paragraphs[0]] + middle_paragraphs + [paragraphs[-1]]\n",
    "    chosen_essay = {\"full_text\": \"\".join([par[\"full_text\"] for par in chosen_paragraphs]),\n",
    "                    \"document\": essay[\"document\"],\n",
    "                    \"tokens\": list(chain(*[par[\"tokens\"] for par in chosen_paragraphs])),\n",
    "                    \"trailing_whitespace\": list(chain(*[par[\"trailing_whitespace\"] for par in chosen_paragraphs])),\n",
    "                    \"labels\": list(chain(*[par[\"labels\"] for par in chosen_paragraphs]))}\n",
    "    return chosen_essay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4573d6fe",
   "metadata": {
    "papermill": {
     "duration": 0.002316,
     "end_time": "2024-05-04T15:47:29.901540",
     "exception": false,
     "start_time": "2024-05-04T15:47:29.899224",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Customized Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08b8ce79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:47:29.908075Z",
     "iopub.status.busy": "2024-05-04T15:47:29.907697Z",
     "iopub.status.idle": "2024-05-04T15:47:29.921104Z",
     "shell.execute_reply": "2024-05-04T15:47:29.920179Z"
    },
    "papermill": {
     "duration": 0.019432,
     "end_time": "2024-05-04T15:47:29.923375",
     "exception": false,
     "start_time": "2024-05-04T15:47:29.903943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TokenizedDataset(Dataset):\n",
    "    def __init__(self, data, fake_data, label2id, max_length, paragraph_augmentation, oversampling):\n",
    "        self.data = data\n",
    "        self.fake_data = fake_data\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(TRAINING_MODEL_PATH)\n",
    "        self.label2id = label2id\n",
    "        self.max_length = max_length\n",
    "        self.paragraph_augmentation = paragraph_augmentation\n",
    "        self.oversampling = oversampling\n",
    "        if oversampling:\n",
    "            self._oversample_pii_essays()\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.oversampling:\n",
    "            return len(self.ds_repeat_indices) + len(self.fake_data)\n",
    "        return len(self.data) + len(self.fake_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        essay = self.get_raw_essay(idx)\n",
    "        if self.paragraph_augmentation:\n",
    "            essay = partial_essay(essay)\n",
    "        self._tokenize(essay)\n",
    "        return essay\n",
    "    \n",
    "    def get_raw_essay(self, idx):\n",
    "        if self.oversampling:\n",
    "            if idx < len(self.ds_repeat_indices):\n",
    "                idx = self.ds_repeat_indices[idx]\n",
    "                essay = self.data[idx].copy()\n",
    "            else:\n",
    "                idx -= len(self.ds_repeat_indices)\n",
    "                essay = self.fake_data[idx].copy()\n",
    "        else:\n",
    "            if idx < len(self.data):\n",
    "                essay = self.data[idx].copy()\n",
    "            else:\n",
    "                idx -= len(self.data)\n",
    "                essay = self.fake_data[idx].copy()\n",
    "        return essay\n",
    "\n",
    "    def _essay_weight(self, essay):\n",
    "        labels_unique = set(essay[\"labels\"])\n",
    "        if not (labels_unique - {'O'}):\n",
    "            return 1\n",
    "        if not (labels_unique - {'I-NAME_STUDENT', 'B-NAME_STUDENT', 'O'}):\n",
    "            return 4\n",
    "        return 8\n",
    "\n",
    "    def _oversample_pii_essays(self):\n",
    "        print(\"Oversampling essays with PIIs\")\n",
    "        self.ds_weights = [self._essay_weight(essay) for essay in self.data]\n",
    "        self.ds_repeat_indices = [i for i, w in enumerate(self.ds_weights) for _ in range(w)]\n",
    "    \n",
    "    def _tokenize(self, essay):\n",
    "        essay[\"provided_labels\"] = essay.pop(\"labels\")\n",
    "        tokenized = tokenize(essay, tokenizer=self.tokenizer, label2id=self.label2id, max_length=self.max_length)\n",
    "        essay.update(tokenized)\n",
    "\n",
    "\n",
    "# ds = TokenizedDataset(data, data, tokenizer, label2id, TRAINING_MAX_LENGTH, paragraph_augmentation=True, oversampling=True)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7500999,
     "sourceId": 66653,
     "sourceType": "competition"
    },
    {
     "datasetId": 4379849,
     "sourceId": 7518925,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12.305144,
   "end_time": "2024-05-04T15:47:30.948934",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-04T15:47:18.643790",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
