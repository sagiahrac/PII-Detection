{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "263e1493",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:51:23.624820Z",
     "iopub.status.busy": "2024-05-04T15:51:23.624488Z",
     "iopub.status.idle": "2024-05-04T15:51:31.623740Z",
     "shell.execute_reply": "2024-05-04T15:51:31.622890Z"
    },
    "papermill": {
     "duration": 8.006361,
     "end_time": "2024-05-04T15:51:31.626135",
     "exception": false,
     "start_time": "2024-05-04T15:51:23.619774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from itertools import chain\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "TRAINING_MODEL_PATH = \"microsoft/deberta-v3-base\"\n",
    "TRAINING_MAX_LENGTH = 1024\n",
    "OUTPUT_DIR = \"output\"\n",
    "STRIDE=384\n",
    "\n",
    "\n",
    "data = json.load(Path('/kaggle/input/pii-detection-removal-from-educational-data/train.json').open(\"r\"))\n",
    "all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n",
    "label2id = {l: i for i,l in enumerate(all_labels)}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d00d83",
   "metadata": {
    "papermill": {
     "duration": 0.002308,
     "end_time": "2024-05-04T15:51:31.631483",
     "exception": false,
     "start_time": "2024-05-04T15:51:31.629175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60c89f9",
   "metadata": {
    "papermill": {
     "duration": 0.002331,
     "end_time": "2024-05-04T15:51:31.636312",
     "exception": false,
     "start_time": "2024-05-04T15:51:31.633981",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will use `DebertaV3` as our primary model. To change the classes weights, we will have to edit training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90ce94f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:51:31.642802Z",
     "iopub.status.busy": "2024-05-04T15:51:31.642383Z",
     "iopub.status.idle": "2024-05-04T15:51:34.866536Z",
     "shell.execute_reply": "2024-05-04T15:51:34.865510Z"
    },
    "papermill": {
     "duration": 3.23025,
     "end_time": "2024-05-04T15:51:34.868952",
     "exception": false,
     "start_time": "2024-05-04T15:51:31.638702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DebertaV2ForTokenClassification\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from typing import Optional, Tuple, Union\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import copy\n",
    "import warnings\n",
    "\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "from transformers.dynamic_module_utils import get_class_from_dynamic_module, resolve_trust_remote_code\n",
    "from transformers.utils import (\n",
    "    CONFIG_NAME,\n",
    "    cached_file,\n",
    "    extract_commit_hash,\n",
    "    find_adapter_config_file,\n",
    "    is_peft_available,\n",
    ")\n",
    "from transformers.models.auto.configuration_auto import AutoConfig\n",
    "\n",
    "\n",
    "class DebertaV2ForTokenClassificationPII(DebertaV2ForTokenClassification):\n",
    "    customized_ce_weights = torch.Tensor([(id2label[id] != 'O') + (id2label[id] == 'O')*0.0009 for id in range(len(id2label))]).to(\"cuda\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, TokenClassifierOutput]:\n",
    "        outs = super().forward(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            labels=None,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss(weight=self.customized_ce_weights)\n",
    "            loss = loss_fct(outs.logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        if not return_dict:\n",
    "            output = (outs.logits,) + outs.outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "        else:\n",
    "            return TokenClassifierOutput(loss=loss, logits=outs.logits, hidden_states=outs.hidden_states, attentions=outs.attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d961043b",
   "metadata": {
    "papermill": {
     "duration": 0.002444,
     "end_time": "2024-05-04T15:51:34.874119",
     "exception": false,
     "start_time": "2024-05-04T15:51:34.871675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdf592b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:51:34.880993Z",
     "iopub.status.busy": "2024-05-04T15:51:34.880564Z",
     "iopub.status.idle": "2024-05-04T15:51:34.899988Z",
     "shell.execute_reply": "2024-05-04T15:51:34.898944Z"
    },
    "papermill": {
     "duration": 0.025356,
     "end_time": "2024-05-04T15:51:34.901983",
     "exception": false,
     "start_time": "2024-05-04T15:51:34.876627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs):\n",
    "        config = kwargs.pop(\"config\", None)\n",
    "        trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n",
    "        kwargs[\"_from_auto\"] = True\n",
    "        hub_kwargs_names = [\n",
    "            \"cache_dir\",\n",
    "            \"force_download\",\n",
    "            \"local_files_only\",\n",
    "            \"proxies\",\n",
    "            \"resume_download\",\n",
    "            \"revision\",\n",
    "            \"subfolder\",\n",
    "            \"use_auth_token\",\n",
    "            \"token\",\n",
    "        ]\n",
    "        hub_kwargs = {name: kwargs.pop(name) for name in hub_kwargs_names if name in kwargs}\n",
    "        code_revision = kwargs.pop(\"code_revision\", None)\n",
    "        commit_hash = kwargs.pop(\"_commit_hash\", None)\n",
    "        adapter_kwargs = kwargs.pop(\"adapter_kwargs\", None)\n",
    "\n",
    "        token = hub_kwargs.pop(\"token\", None)\n",
    "        use_auth_token = hub_kwargs.pop(\"use_auth_token\", None)\n",
    "        if use_auth_token is not None:\n",
    "            warnings.warn(\n",
    "                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n",
    "                FutureWarning,\n",
    "            )\n",
    "            if token is not None:\n",
    "                raise ValueError(\n",
    "                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n",
    "                )\n",
    "            token = use_auth_token\n",
    "\n",
    "        if token is not None:\n",
    "            hub_kwargs[\"token\"] = token\n",
    "\n",
    "        if commit_hash is None:\n",
    "            if not isinstance(config, PretrainedConfig):\n",
    "                # We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\n",
    "                resolved_config_file = cached_file(\n",
    "                    pretrained_model_name_or_path,\n",
    "                    CONFIG_NAME,\n",
    "                    _raise_exceptions_for_gated_repo=False,\n",
    "                    _raise_exceptions_for_missing_entries=False,\n",
    "                    _raise_exceptions_for_connection_errors=False,\n",
    "                    **hub_kwargs,\n",
    "                )\n",
    "                commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n",
    "            else:\n",
    "                commit_hash = getattr(config, \"_commit_hash\", None)\n",
    "\n",
    "        if is_peft_available():\n",
    "            if adapter_kwargs is None:\n",
    "                adapter_kwargs = {}\n",
    "                if token is not None:\n",
    "                    adapter_kwargs[\"token\"] = token\n",
    "\n",
    "            maybe_adapter_path = find_adapter_config_file(\n",
    "                pretrained_model_name_or_path, _commit_hash=commit_hash, **adapter_kwargs\n",
    "            )\n",
    "\n",
    "            if maybe_adapter_path is not None:\n",
    "                with open(maybe_adapter_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    adapter_config = json.load(f)\n",
    "\n",
    "                    adapter_kwargs[\"_adapter_model_path\"] = pretrained_model_name_or_path\n",
    "                    pretrained_model_name_or_path = adapter_config[\"base_model_name_or_path\"]\n",
    "\n",
    "        if not isinstance(config, PretrainedConfig):\n",
    "            kwargs_orig = copy.deepcopy(kwargs)\n",
    "            # ensure not to pollute the config object with torch_dtype=\"auto\" - since it's\n",
    "            # meaningless in the context of the config object - torch.dtype values are acceptable\n",
    "            if kwargs.get(\"torch_dtype\", None) == \"auto\":\n",
    "                _ = kwargs.pop(\"torch_dtype\")\n",
    "            # to not overwrite the quantization_config if config has a quantization_config\n",
    "            if kwargs.get(\"quantization_config\", None) is not None:\n",
    "                _ = kwargs.pop(\"quantization_config\")\n",
    "\n",
    "            config, kwargs = AutoConfig.from_pretrained(\n",
    "                pretrained_model_name_or_path,\n",
    "                return_unused_kwargs=True,\n",
    "                trust_remote_code=trust_remote_code,\n",
    "                code_revision=code_revision,\n",
    "                _commit_hash=commit_hash,\n",
    "                **hub_kwargs,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "            # if torch_dtype=auto was passed here, ensure to pass it on\n",
    "            if kwargs_orig.get(\"torch_dtype\", None) == \"auto\":\n",
    "                kwargs[\"torch_dtype\"] = \"auto\"\n",
    "            if kwargs_orig.get(\"quantization_config\", None) is not None:\n",
    "                kwargs[\"quantization_config\"] = kwargs_orig[\"quantization_config\"]\n",
    "\n",
    "        has_remote_code = False\n",
    "        has_local_code = True\n",
    "        trust_remote_code = resolve_trust_remote_code(\n",
    "            trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code\n",
    "        )\n",
    "\n",
    "        # Set the adapter kwargs\n",
    "        kwargs[\"adapter_kwargs\"] = adapter_kwargs\n",
    "\n",
    "        return DebertaV2ForTokenClassificationPII.from_pretrained(\n",
    "            pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n",
    "        )\n",
    "\n",
    "# model = from_pretrained(\n",
    "#     TRAINING_MODEL_PATH,\n",
    "#     num_labels=len(all_labels),\n",
    "#     id2label=id2label,\n",
    "#     label2id=label2id,\n",
    "#     ignore_mismatched_sizes=True\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7500999,
     "sourceId": 66653,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16.136118,
   "end_time": "2024-05-04T15:51:37.062141",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-04T15:51:20.926023",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
